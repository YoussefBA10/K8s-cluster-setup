# Cluster Initialization & Networking

This document records the setup of the Kubernetes control plane and the integration of the worker nodes.

## Environment Summary
- **Nodes:** 1 Master (`vmpipe`), 2 Workers (`node-1`, `node-2`)
- **K8s Version:** v1.29.15
- **CNI Plugin:** Calico (Enterprise-grade networking & policy)

## Step 1: Control Plane Initialization
Initialized via `kubeadm` with the following CIDR to support Calico/Flannel defaults:
```bash
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
Step 2: Node Joins
Worker nodes joined using the token generated by the master.

Verification: kubectl get nodes (Initially showed NotReady)

Step 3: Calico Networking (CNI)
We deployed Calico using the Tigera Operator for better lifecycle management.

Installation Steps:
Apply Operator: Installs the Tigera controller.

Apply Custom Resources: Defines the IP Pool and encapsulation (VXLAN).

CIDR Alignment: Modified the default 192.168.0.0/16 to 10.244.0.0/16 in the manifest to match the cluster init.

Verification Command:
Bash
kubectl get pods -n calico-system
Note: Nodes will remain NotReady until the calico-node pods are Running on every host.


---

### üõ†Ô∏è Troubleshooting Tip
If the status doesn't change after 5 minutes, run:
```bash
kubectl describe node node-1 | grep Network
